---
title: "Assignments week 2 for Classical Methods in Data Analysis"
author: "Wouter van Amsterdam"
date: 2017-10-29
output: html_document
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("chunks.R")
```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}
```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}
```

<!-- Add your analysis here -->

## Day 5: ANOVA and non-parametric methods
### Exercises without SPSS or R

#### 1.	
> Twelve plots of land were randomly divided into three groups of four plots. Two fertilizers, A and B, are applied for the cultivation of wheat while the third group C serves as a control group, without application of fertilizer. The wheat yields are as follows:

```{r}
yields <- matrix(c(72, 74, 60, 
          67, 78, 64,
          60, 72, 65, 
          66, 68, 55),
          nrow = 4, byrow = T,
       dimnames = list(1:4, c("A", "B", "C")))
yields
```

> a.	How would you perform the randomization for this study?

Randomize assignment to fertilizers or control. Perform double blinding, including 'placebo' fertilizer.

> The following ANOVA table was produced on the basis of data above:

> Sum of Squares	df	Mean Square	F	p-value
> Between Groups	289.50	?	?	?	0.015
> Within Groups	186.75	?	?		
> Total	476.25	?			

> b.	Fill in the missing parts of the ANOVA table. See if you can find the p-value in the F tables.

Recall that:

$$df_{total} = \sum{(n_j)}-1$$
$$df_{within} = \sum({n_j-1})$$
$$df_{between} = J-1$$

Here $J = 3$ the number of groups.
 
```{r}
n_total = length(yields)
n_groups = ncol(yields)
n_pergroup = nrow(yields)

sum_squares <- function(x) sum((x-mean(x))^2)

ss_total = sum_squares(as.vector(yields))
ss_within = sum(apply(yields, 2, sum_squares)) # this applies 'sum_squares' to each column of yields
ss_between = sum(n_pergroup*((colMeans(yields) - mean(yields))^2))

df_total = n_total - 1
df_within = 3*(n_pergroup - 1)
df_between = n_groups - 1

ms_within = ss_within / df_within
ms_between = ss_between / df_between

f_value = ms_between / ms_within

qf(p = 0.95, df1 = df_between, df2 = df_within)

```

So here $a = `r df_between`$ and $b = `r df_within`$.

> c.	Test the null hypothesis that there is no difference among the three treatments A, B and C, meaning that the use of fertilizers under the conditions of the experiment has no effect.

```{r}
pf(q = f_value, df1 = df_between, df2 = df_within, lower.tail = F)
```


> d.	If no significant overall result had been found by ANOVA, would the result of multiple post-hoc tests be valid?

No, this is only meaningful when there is an overall effect found.

Now replicate with built-in anova:
```{r}
yields_melted = data.frame(group = rep(c("A", "B", "C"), each = 4),
                          yield = as.vector(yields))
yields_melted
summary(aov(yield~group, data = yields_melted))
```


#### 2.	
> The aim of a certain survey in a hospital was to compare the waiting times of patients in two clinics. The waiting times (in minutes) were as follows: 

```{r}
load(amstR::fromParentDir("data/clinic.RData"))
summary(clinic)
```

Looks like there is a row with only missing values, let's remove that row.
```{r}
clinic[is.na(clinic$TIME),]
full_clinic <- clinic
clinic <- clinic[!is.na(clinic$TIME),]
```

Create plots of the densities
```{r}
require(ggplot2)
ggplot(clinic, aes(x = TIME, fill = CLINIC)) + 
  geom_density(alpha = .7)
```

Both distributions of waiting times seem approximately normally distributed, with some right skewness and (of course) a lower bound of (>)0.

> What are the problems when you compare these data? What type of analysis would you prefer? Which questions exactly are answered with your analysis? What would you do if some waiting times were ‘censored’ because patients left the hospital? The data are given in the file clinic.sav or clinic.RData, if you want to easily get some descriptive statistics to help you answer the questions above.

Both groups are of unequal size, and their variances seem to differ. This will cause troubles with ANOVA as it violates homoscedasticity. Since these are left-bounded varaibles, they seem approximately poisson-distributed. Currently we have no tools to deal with censored data. The most straightforward approach seems to be to do a Welch unpaired two-sample T-test (Welch = no equal variance assumption). We could look at the Wilcoxon rank sum test or the Mann-Whitney test. These tests do not require normality, however they do require homoscedasticity. Looking at the distributions of our data, heteroscedasticity seems to be a bigger problem than non-normality.

```{r}
t.test(TIME ~ CLINIC, data = clinic, paired = F, var.equal = F)
```

Check distribution of residuals
```{r}
fit <- lm(TIME~CLINIC, data = clinic)
plot(fit, which = 2)
```

The residuals are higher than expected on the upper side of the distribution, which indicates right skewed data, as could be seen from the density plots. 


### Exercises in R

#### 9.	
> Answer the research questions from exercise #1, but now in R (use dataset wheat.RData).
> a.	Make sure to perform a complete analysis, including checking the model assumptions. 

Perform ANOVA
```{r}
load(amstR::fromParentDir("data/wheat.RData"))
fit <- aov(YIELD~FERTILIZ, data = wheat)
summary(fit)
pairwise.t.test(wheat$YIELD, wheat$FERTILIZ, p.adj = "none")
```

Dunnay post-hoc analysis (comparing with a basline) is not available in R
We used no p-value adjustment, since only two comparisons were performed that are of interest: A vs C and B vs C. However, we could apply Bonferroni adjustment by taking these p-values and multiplying them by 2 (instead of 3 for aforementioned reasons).

Check assumptions:
Normal distribution of residuals
Recall that the `lm` function is used for linear regression. In our case of a interval outcome variable and a categorical independent variable, it calculates the mean for each level of our independent variable. The resulting residuals are the difference between the observed value and the mean for the corresponding group. So these are exactly the same as for the ANOVA method, and can be used to easily get the residuals from our model.
```{r}
fit <- lm(YIELD~FERTILIZ, data = wheat)
plot(fit, which = c(1,2))
```

* From the first plot (fitted vs residuals), we can see that the spread is more or less the same for each group, so homoscedasticity seems to hold. 
* From the second plot, we see that the residuals fit the normal distribution quite OK, exept for the lower tail, where it seems that the actual values are a little lower than what would be expected from a normal distribution. 
* Independence of observations can not be checked from the data, but should follow from the study design

> b.	How does the output for this analysis compare to the output for the same analysis in SPSS?

The same F-value and p-value are calculated. Comparing with SPSS, this table does not include a row for 'total'.

#### 10. Starfish
Answer the research questions from exercise #3, but now in R (use dataset starfish.RData).

> 3.	To study the reproductive cycle of a specific species of starfish, individuals from two different locations were observed. To check whether the populations of starfishes at the two places differ in the mean metabole, two random samples were compared. The data has been saved in the file starfish.sav.

> a.	Perform an independent samples t-test to test whether the mean metabole differs between the two locations and comment on the result.

```{r}
load(amstR::fromParentDir("data/starfish.RData"))
t.test(metabole~location, data = starfish, var.equal = F, paired = F)
t.test(metabole~location, data = starfish, var.equal = T, paired = F)
```

> b.	Also perform a one-way analysis of variance and compare this with the result from a). 

```{r}
summary(aov(metabole~location, data = starfish))
```

For a single independent variable with only 2-levels, the one-way ANOVA is equivalent to the unpaired two-sample T-test, assuming equal variance.

> c.	What is the relation between the t-value of the t-test and the F-value of the ANOVA?

```{r}
t.test(metabole~location, data = starfish, var.equal = T, paired = F)$statistic^2
```

The F-value is the squared t-value.

#### 11. Sleep
> This exercise uses the built-in dataset sleep.
> a.	Load the built-in dataset sleep into your workspace: data(sleep). (Type help(sleep) for information on the study design.)

```{r}
data(sleep)
```

> b.	Make a boxplot and QQ-plot of the difference in extra hours of sleep for the two drugs for each patient.

Note that these are paired data

```{r}
diff = sleep[sleep$group == "1", "extra"] - sleep[sleep$group == "2", "extra"]
boxplot(diff)
qqnorm(diff)
qqline(diff, col = "red")
```

Looks pretty normally distributed.

> c.	Which parametric test would you want to use to detect a difference in sleep for the two drugs? Is this test allowed? Would a non-parametric test be allowed here?

A paired t-test could be used. The differences look pretty normally distributed. Otherwiss the Wilcoxon signed rank test could be used, assuming independent samples.

```{r}
t.test(extra~group, data = sleep)
wilcox.test(extra~group, data = sleep)
```

> 12.	Open the dataset water.RData. The dataset describes mortality and drinking water hardness for 61 cities in England and Wales. The column mortality is the averaged annual mortality per 100000 male inhabitants and the column hardness represents the calcium concentration (in parts per million). The meaning of the remaining columns is self-explanatory.
a.	Make a boxplot in which the hardness of the water in northern and southern regions is compared.

```{r}
load(amstR::fromParentDir("data/water.RData"))
str(water)
boxplot(hardness~location, data = water)
```

> b.	Compare the hardness of water in northern and southern regions in a t-test. Are the conditions of a t-test met?

```{r}
t.test(hardness~location, data = water, var.equal = F, paired = F)
```

Equal variance does not have to be assumed. Normality of residuals however does need to be assumed. Inspect them with a QQ-plot:

```{r}
plot(lm(hardness~location, data = water), which = 2)
```

Does not look all too bad.

> c.	Could you use a non-parametric test instead of the t-test in part b)?

The variance in the South region is much higher than in the North region, this violates the assumptions from the Wilcoxon rank-sum test and the Mann-Whitney test, so no.

#### 13.	Student incomes
> This is a repeat of exercise #8, but now in R. In the dataset incomes.RData the column income represents a (would-be) sample of incomes for 100 randomly chosen students in Utrecht. The aim of this exercise is to make you aware of the risks involved when using transformation techniques in parametric statistics.
a.	Compute the mean income in this sample.

```{r}
load(amstR::fromParentDir("data/incomes.RData"))
str(incomes)
mean(incomes$income)
```

> b.	Ignoring all conditions, compute a 95% confidence interval for the mean income of all students in Utrecht.

```{r}
t.test(incomes$income)$conf.int
```

> c.	Is the interval found in b) reliable?

This interval should be reliable when the income in the population is normally distributed, or when the central-limit theorem helps us. First take a look at the distribution of our sample

```{r}
hist(incomes$income)
```

These are highly skewed, now it is hard to know whether the central limit theorem will help us. We could try bootstrapping to see if our sample-mean looks T-distributed, but that will go too far for now.

> d.	Transform the column income into a column logincome by taking the logarithms of the values in income.

```{r}
incomes$logincome <- log(incomes$income)
hist(incomes$logincome)
```

Looks much nicer.

> e.	Compute a 95% confidence interval for the mean of log transformed student incomes in Utrecht.

```{r}
t.test(incomes$logincome)$conf.int
```


> f.	On the interval found in e), carry out a backward transformation to obtain an interval estimate for the mean income of all students in Utrecht.

```{r}
exp(t.test(incomes$logincome)$conf.int)
```

> g.	Is the interval found in f) reliable?

```{r}
t.test(incomes$income)$conf.int
exp(t.test(incomes$logincome)$conf.int)
```

The confidence intervals for both methods are pretty different, there is hardly any overlap. Since the log-transformed variable fits the model assumptions better, this result is deemed more reliable

NB todo: create histogram, and plot both confidence intervals and the median in a single plot

Now for a normally distributed variable:

```{r}
set.seed(2)
x <- 10+ rnorm(100)
hist(x)
t.test(x)$conf.int
exp(t.test(log(x))$conf.int)
```


Now check a two sample test for (random) groups
```{r}
require(ggplot2)
incomes$group <- sample(c("A", "B"), p = c(0.5,0.5), size = nrow(incomes), replace = T)
# increase the income of group B a little
incomes$new_income <- incomes$income + as.numeric(incomes$group == "B") * .5 * (median(incomes$income))
incomes$log_new_income <- log(incomes$new_income)

ggplot(incomes, aes(x = new_income, fill = group)) + 
  geom_density(alpha = 0.7)

ggplot(incomes, aes(x = log_new_income, fill = group)) + 
  geom_density(alpha = 0.7)

t.test(new_income ~ group, data = incomes)
t.test(log_new_income ~ group, data = incomes)

```

The T-test seems to have higher power in for the log-transformed income.

## Day 6: Correlation and regression
> Start every exercise by thinking about the research question(s), the research design, the dependent and explanatory variables, the descriptive statistics and the analysis plan. Discuss these issues with your fellow students. The further you are in the course, the easier it should be to identify the research question and the proper approach(es) to answering that question.

### Exercises without SPSS or R

#### 1.
> In a sample of 10 animals of a certain species the lengths of the right foreleg and the right hind leg have been measured. 

```{r}
y1 = c(54, 53, 58, 55, 56, 55, 56, 57, 53, 57) # Foreleg
y2 = c(56, 55, 57, 57, 56, 58, 59, 59, 56, 58) # Hind leg
```

> The mean (standard deviation) for both legs are 55.4 (1.71) and 57.1 (1.37) respectively and the covariance is 1.51. Study the association between the length of the foreleg and the hind leg. 
a.	Would you prefer correlation or regression?

Correlation would be preferred, as there is no a priori reason to believe that 
one is cause by the other, or should predict the other.

> b.	Make a scatter plot. Try to guess the value of the Pearson’s correlation coefficient.

```{r}
plot(y1, y2)
```

A guess would be around 0.5

> c.	Calculate  the Pearson’s correlation coefficient and its standard error. Check the significance. What does the significance mean?

```{r}
n = length(y1)
r = sum((y1 - mean(y1))*(y2 - mean(y2))) / sqrt((sum_squares(y1)*sum_squares(y2)))
r
cor(y1, y2)

se_r = sqrt((1-r^2)/(n-2))
se_r

t = r / se_r
2*pt(q = t, df = n-2, lower.tail = F)
cor.test(y1, y2)
```

> d.	Calculate a 95% confidence interval for the correlation coefficient, using Fisher's Z transformation procedure. 

```{r}
z = .5*log((1+r)/(1-r))
lo = z - qnorm(p = 0.025)/sqrt(n-3)
hi = z + qnorm(p = 0.025)/sqrt(n-3)
lo_r = (exp(2*lo)-1)/(exp(2*lo)+1)
hi_r = (exp(2*hi)-1)/(exp(2*hi)+1)
lo_r; hi_r
cor.test(y1, y2)$conf.int
```

### Exercises with R
#### 6.
> In a sample of 10 animals of a certain species the lengths of the foreleg and the hind leg have been measured. The data are also given in the dataset `legs.RData`. Study the association between the length of the foreleg and the hind leg. 

```{r}
load(amstR::fromParentDir("data/legs.RData"))
plot(y1, y2)
cor.test(y1, y2)

qqnorm(y1); qqline(y1, col = "red")
qqnorm(y2); qqline(y2, col = "red")
```

There is a statistically significant association between the foreleg and the hind leg in our data.

#### 7. 
> Get the data in the file infantmortality.txt in R. The dataset is about infant mortality in the USA. The first column in the dataset presents the state of the USA to which the data applies; name it therefore state. The second column presents the teenage birth rate per 1000 births and could be named teen. The third column presents the infant mortality rate per 1000 live births; name this column mort. In the USA there is a conjecture that infant mortality is in many cases caused by teenage mothers who do not receive proper prenatal care. 
a.	Perform a linear regression with teen as explanatory variable and mort as the response variable. 

```{r}
infants <- read.table(amstR::fromParentDir("data/infantmortality.txt"), header = F)
colnames(infants) <- c("state", "teen", "mort")
head(infants)
plot(mort~teen, data = infants)
fit <- lm(mort~teen, data = infants)
summary(fit)
```

> b.	Do not forget to check the regression conditions!

```{r}
plot(fit, which = c(1,2))
```

From plot 1: the relationship between `teen` and `mort` seems more or less linear.
From plot 2: variance seems equal across all levels of `teen`. 
From plot 3: residuals look OK normally distributed, with a little heavy tail on the left.

Independence of observations cannot be checked from the data, but is probable due to the study design.

> c.	Do your results in a) confirm the conjecture?

Yes, they provide evidence that with increasing amount of teenage pregnancy, infant mortality rises. 
However, whether or not this is a causal relationship cannot be judged from the data alone. 
For instance, there may be another unmeasured variable that explains the observed association.

> d.	Calculate two 95% confidence intervals: one for the infant mortality for cases in which the teenage birth rate is equal to the sample mean, and one for an individual case with teenage birth rate equal to the sample mean. Comment on the difference between those intervals.

```{r}
# by 'hand' 
n = nrow(infants)
se = sd(fit$residuals)
y0 = fit$coefficients[1] + fit$coefficients[2] * mean(infants$teen)
## mean prediction
y0 + c(1,-1) * qt(df = n-1, p = .025, lower.tail = T)  * se * sqrt(1/n)
## individual prediction
y0 + c(1,-1) * qt(df = n-1, p = .025, lower.tail = T)  * se * sqrt(1+1/n)


predict(fit, newdata = data.frame(teen = mean(infants$teen)), type = "response", interval = "confidence", level = 0.95)
predict(fit, newdata = data.frame(teen = mean(infants$teen)), type = "response", interval = "prediction", level = 0.95)
```

Calculating a confidence interval for the mean predicted infant mortality for a certain level of `teen` with our model is dependent on the 
model variance $\sigma_{\epsilon}$ and the location of the predictor (closer or further away from the mean). It is calculated with the function:

$$\hat{y}_0 \pm t_{(n-2, \alpha/2)}\sigma_{\epsilon}\sqrt{\frac{1}{n} + \frac{(x_0-\bar{x})}{\sum_{i = 1}^n{(x_i-\bar{x})^2}}}$$

Predicting the risk of mortality for an individual for a certain level of `teen` also requires to take into account the level of spread at that level of `teen` (although this spread is assumed to be equal for each value of `teen`...), so it is a wider confidence interval. It is calculated with the function:

$$\hat{y}_0 \pm t_{(n-2, \alpha/2)}\sigma_{\epsilon}\sqrt{1 + \frac{1}{n} + \frac{(x_0-\bar{x})}{\sum_{i = 1}^n{(x_i-\bar{x})^2}}}$$

Note that the hand-calculated confidence intervals differ slightly from the confidence intervals which r gives us.

#### 8.
> In the file `heparin.RData` the clotting times of blood samples are registered for different doses of heparin. Research is directed towards the relationship between these quantities.
> a.	Perform a regression analysis and check whether the assumptions of normality, linearity and homoscedasticity are fulfilled.

```{r}
load(amstR::fromParentDir("data/heparin.RData"))
plot(TIME~DOSE, data = heparine)
fit <- lm(TIME~DOSE, data = heparine)
plot(fit, which = c(1,2))

```

From plot 1: the relationship seems more or less linear
From plot 2: there is clear heteroscedasticity: variance increases with DOSE
From plot 3: the residuals look somewhat normally distributed, with heavier tails

> b.	A logarithmic transformation on the dependent variable is often proposed when assump¬tions are not fulfilled. Check whether this transformation leads to fulfilment of the three assumptions.

```{r}
heparine$log_time <- log(heparine$TIME)
plot(log_time~DOSE, data = heparine)
fit2 <- lm(log_time~DOSE, data = heparine)
plot(fit2, which = c(1,2))
```

From plot 1: Linearity seems to be less in the log-transformed time
From plot 2: The spread in residuals is relatively equal accross levels of `DOSE`, so homoscedasticity seems to hold
From plot 3: residuals are nicely normally distribueted

> c.	Carry out another regression analysis in which both variables are logarithmically transformed. Check again the assumptions and conclude on the results. 

```{r}
heparine$log_dose <- log(heparine$DOSE)
plot(log_time~log_dose, data = heparine)
fit3 <- lm(log_time~log_dose, data = heparine)
plot(fit3, which = c(1,2))
```

Everything looks better when both variables are log-transformed!

### Challenge exercise
##### 9.
> The question arises how body mass index (BMI) is related to the process of gastric emptying, measured by the percentage of retention after 120 minutes (ret120). The data of 55 patients of a certain hospital can be found in the file diabetes.sav or diabetes.RData.
a.	Make a scatter plot of BMI and ret120 and give an interpretation of the correlation coefficient and its significance.

```{r}
load(amstR::fromParentDir("data/diabetes.RData"))
plot(ret120~bmi, data = diabetes.df)
cor.test(~ret120+bmi, data = diabetes.df)
```

Based on the scatter plot, there seems to be no correlation between `bmi` and `ret120`.
It is therefore not surprising that the correlation-coefficient is not statistically significantly different from 0.

> b.	The file consists of two groups of patients, diabetes and non-diabetes. Try to make a scatter plot in which the two groups can be distinguished 

```{r}
require(ggplot2)
ggplot(diabetes.df, aes(x = bmi, y = ret120, col = diabetes, shape = diabetes)) + 
  geom_point()
```

> c.	Calculate the correlation coefficient for the two groups separately and give your conclusion

```{r}
cor.test(~ret120+bmi, data = diabetes.df[diabetes.df$diabetes=="Yes",])
cor.test(~ret120+bmi, data = diabetes.df[diabetes.df$diabetes=="No",])
```

It looks like there is an association between `bmi` and `ret120`, but is different 
in the group with diabetes from the patients without diabetes. In a model this can be seen as interaction.

```{r}
fit0 <- lm(ret120~bmi, data = diabetes.df)
summary(fit0)
plot(fit0, which = c(1,2))
fit1 <- lm(ret120~bmi*diabetes, data = diabetes.df)
summary(fit1)
plot(fit1, which = c(1,2))
```

Adding `diabetes` as an interaction term in model 1 is the same as making two models, one for each group:
```{r}
fit_no = lm(ret120~bmi, data = diabetes.df[diabetes.df$diabetes == "No",])
summary(fit_no)
fit_yes = lm(ret120~bmi, data = diabetes.df[diabetes.df$diabetes == "Yes",])
summary(fit_yes)
```


Is model 1 better than model 0?
```{r}
anova(fit0, fit1)
```

Yes

To interpret the coefficients of fit1:

for patients without diabetes:
$$ret120 = 61.422 - 0.8323 * bmi$$
for patients with diabetes:
$$ret120 = 61.422 - 46.3974 + (0.8323 + 1.1371)*bmi$$

In a plot

```{r}
# some plot parameters
xmin = floor(min(diabetes.df$bmi))
xmax = ceiling(max(diabetes.df$bmi))
ymin = floor(min(diabetes.df$ret120))
ymax = ceiling(max(diabetes.df$ret120))

par(mfrow = c(1,2))


plot(ret120~bmi, data = diabetes.df[diabetes.df$diabetes == "No",], 
     xlim = c(xmin, xmax), ylim = c(ymin, ymax), main = "Without diabetes")
abline(a = fit1$coefficients["(Intercept)"], b = fit1$coefficients["bmi"], col = "red")

plot(ret120~bmi, data = diabetes.df[diabetes.df$diabetes == "Yes",], 
     xlim = c(xmin, xmax), ylim = c(ymin, ymax), main = "With diabetes")
abline(a = (fit1$coefficients["(Intercept)"] + fit1$coefficients["diabetesYes"]), 
       b = (fit1$coefficients["bmi"] + fit1$coefficients["bmi:diabetesYes"]), col = "red")
par(mfrow = c(1,1))
```

This plot can be made very easily with ggplot2
```{r}
require(ggplot2)
ggplot(diabetes.df, aes(x = bmi, y = ret120)) + # define dataset, x and y variable
  geom_point() + # add points to the plot, making this a scatterplot
  geom_smooth(method = "lm") + # add linear regression line
  facet_wrap(~diabetes) # make this plot for all levels of the factor variable `diabetes`
```


## Session information

<!-- Insert the session information into the document -->
```{r session-info}
```
